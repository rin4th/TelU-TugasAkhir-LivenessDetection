{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPohYQWaYKT-"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqccBA1eYNAZ"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDnrSaO-kWnw"
   },
   "source": [
    "# Cek GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import importlib\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V3_Small_Weights\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xv848kuokXWW"
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OWL-n7ykqJC"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpRmQB4xktgd"
   },
   "source": [
    "## Jumlah data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkqVlmKDkrc-"
   },
   "outputs": [],
   "source": [
    "def count_images_in_folder(folder_path):\n",
    "    \"\"\"Menghitung jumlah gambar dalam folder dan subfolder.\"\"\"\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        count += len([f for f in files if f.endswith(('.jpg', '.jpeg', '.png'))]) \n",
    "    return count\n",
    "\n",
    "# Path ke folder datasets\n",
    "training_path = \"datasets/training\"\n",
    "testing_path = \"datasets/testing\"\n",
    "\n",
    "# Hitung jumlah gambar di folder training\n",
    "training_count = count_images_in_folder(training_path)\n",
    "print(f\"Jumlah gambar di folder training: {training_count}\")\n",
    "\n",
    "# Total jumlah gambar di folder testing\n",
    "total_testing_count = count_images_in_folder(testing_path)\n",
    "print(f\"Jumlah gambar di folder testing: {total_testing_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-RiI8URlBkl"
   },
   "source": [
    "## Distribusi datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSlkexvPlCoF"
   },
   "outputs": [],
   "source": [
    "def count_images_in_classes(folder_path):\n",
    "    \"\"\"Menghitung jumlah gambar dalam setiap kelas di folder.\"\"\"\n",
    "    class_counts = {}\n",
    "    for class_folder in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_folder)\n",
    "        if os.path.isdir(class_path):  # Pastikan hanya menghitung folder (kelas)\n",
    "            class_counts[class_folder] = len([\n",
    "                f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ])\n",
    "    return class_counts\n",
    "\n",
    "def plot_distribution(class_counts, title):\n",
    "    \"\"\"Menampilkan distribusi dataset menggunakan grafik batang.\"\"\"\n",
    "    classes = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(classes, counts)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Path ke folder training dan testing\n",
    "home_path = \"/workspace/TelU-TugasAkhir-LivenessDetection\"\n",
    "training_path = f\"{home_path}/datasets/training\"\n",
    "testing_path = f\"{home_path}/datasets/testing\"\n",
    "\n",
    "# Hitung distribusi kelas untuk training\n",
    "training_class_counts = count_images_in_classes(training_path)\n",
    "print(f\"Distribusi dataset untuk training: {training_class_counts}\")\n",
    "plot_distribution(training_class_counts, \"Training Dataset Distribution\")\n",
    "\n",
    "# Hitung distribusi kelas untuk testing\n",
    "testing_class_counts = count_images_in_classes(testing_path)\n",
    "print(f\"Distribusi dataset untuk testing: {testing_class_counts}\")\n",
    "plot_distribution(testing_class_counts, \"Testing Dataset Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_images(folder_path, title, samples_per_class=5):\n",
    "    classes = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Setup grid untuk menampilkan gambar\n",
    "    fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(samples_per_class * 3, num_classes * 3))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        class_path = os.path.join(folder_path, cls)\n",
    "        images = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        sample_images = images[:samples_per_class]  # Ambil sejumlah gambar untuk preview\n",
    "\n",
    "        for j in range(samples_per_class):\n",
    "            ax = axes[i, j] if num_classes > 1 else axes[j]  # Jika hanya 1 kelas, axes menjadi 1D\n",
    "            ax.axis('off')\n",
    "\n",
    "            if j < len(sample_images):  # Jika ada gambar yang cukup\n",
    "                img_path = os.path.join(class_path, sample_images[j])\n",
    "                img = Image.open(img_path)\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(cls if j == 0 else \"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Path ke folder training dan testing\n",
    "training_path = f\"{home_path}/datasets/training\"\n",
    "testing_path = f\"{home_path}/datasets/testing\"\n",
    "\n",
    "# Preview gambar di folder training\n",
    "preview_images(training_path, \"Training Dataset Preview\", samples_per_class=5)\n",
    "\n",
    "# Preview gambar di folder testing\n",
    "preview_images(testing_path, \"Testing Dataset Preview\", samples_per_class=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung jumlah parameter\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Membuat dictionary untuk setiap model EfficientNetV2 dan menghitung jumlah parameternya\n",
    "list_models = {\n",
    "    \"EfficientNetV2_Small\": models.efficientnet_v2_s(weights=None),\n",
    "    \"EfficientNetV2_Medium\": models.efficientnet_v2_m(weights=None),\n",
    "    \"EfficientNetV2_Large\": models.efficientnet_v2_l(weights=None),\n",
    "}\n",
    "\n",
    "# Menyimpan hasil dalam dictionary\n",
    "param_counts = {model_name: count_params(model) for model_name, model in list_models.items()}\n",
    "\n",
    "# Memformat jumlah parameter dengan titik sebagai pemisah ribuan\n",
    "formatted_param_counts = {model_name: f\"{count:,}\".replace(\",\", \".\") for model_name, count in param_counts.items()}\n",
    "\n",
    "# Menampilkan hasil dalam tabel (pandas DataFrame)\n",
    "param_df = pd.DataFrame(list(formatted_param_counts.items()), columns=[\"Model\", \"Parameter Count\"])\n",
    "\n",
    "# Menampilkan tabel\n",
    "param_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung jumlah parameter\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Membuat dictionary untuk setiap model Swin Transformer dan menghitung jumlah parameternya\n",
    "list_models = {\n",
    "    \"Swin-Tiny\": models.swin_t(weights=\"IMAGENET1K_V1\"),\n",
    "    \"Swin-Small\": models.swin_s(weights=\"IMAGENET1K_V1\"),\n",
    "    \"Swin-Base\": models.swin_b(weights=\"IMAGENET1K_V1\")\n",
    "}\n",
    "\n",
    "# Menyimpan hasil dalam dictionary\n",
    "param_counts = {model_name: count_params(model) for model_name, model in list_models.items()}\n",
    "\n",
    "# Memformat jumlah parameter dengan titik sebagai pemisah ribuan\n",
    "formatted_param_counts = {model_name: f\"{count:,}\".replace(\",\", \".\") for model_name, count in param_counts.items()}\n",
    "\n",
    "# Menampilkan hasil dalam tabel (pandas DataFrame)\n",
    "param_df = pd.DataFrame(list(formatted_param_counts.items()), columns=[\"Model\", \"Parameter Count\"])\n",
    "\n",
    "# Menampilkan tabel\n",
    "param_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f\"{home_path}/datasets/\"\n",
    "save_model_path = f\"{home_path}/results/best_model/\"\n",
    "result_csv_path = f\"{home_path}/results/csv/\"\n",
    "image_path = f\"{home_path}/results/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories\n",
    "train_dir = os.path.join(dataset_path, \"training\")\n",
    "val_dir = os.path.join(dataset_path, \"validation\")\n",
    "test_dir = os.path.join(dataset_path, \"testing\")\n",
    "\n",
    "# Random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs_size = 30\n",
    "optimizer = \"Adam\"\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "trainable_percentage = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        return 0\n",
    "    return sum(len(files) for _, _, files in os.walk(folder_path) if files)\n",
    "\n",
    "# Fungsi untuk melakukan split\n",
    "def split_training_data(train_dir, val_dir, validation_split=0.1):\n",
    "    if os.path.exists(val_dir):\n",
    "        print(f\"Folder '{val_dir}' sudah ada. Tidak melakukan split ulang.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterasi setiap kelas di folder training\n",
    "    for class_name in os.listdir(train_dir):\n",
    "        class_train_path = os.path.join(train_dir, class_name)\n",
    "        class_val_path = os.path.join(val_dir, class_name)\n",
    "\n",
    "        if not os.path.isdir(class_train_path):\n",
    "            continue\n",
    "        \n",
    "        os.makedirs(class_val_path, exist_ok=True)\n",
    "\n",
    "        # Ambil semua file gambar di kelas ini\n",
    "        all_images = [f for f in os.listdir(class_train_path) if os.path.isfile(os.path.join(class_train_path, f))]\n",
    "        total_images = len(all_images)\n",
    "\n",
    "        # Pilih 10% gambar secara acak untuk dipindahkan ke validation\n",
    "        val_size = int(total_images * validation_split)\n",
    "        val_images = random.sample(all_images, val_size)\n",
    "\n",
    "        # Pindahkan gambar ke folder validation\n",
    "        for img in val_images:\n",
    "            src_path = os.path.join(class_train_path, img)\n",
    "            dest_path = os.path.join(class_val_path, img)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "# Hitung jumlah gambar sebelum split\n",
    "print(\"Sebelum split:\")\n",
    "train_count_before = count_images_in_folder(train_dir)\n",
    "val_count_before = count_images_in_folder(val_dir)\n",
    "print(f\"  Training: {train_count_before} gambar\")\n",
    "print(f\"  Validation: {val_count_before} gambar\")\n",
    "\n",
    "# Lakukan split\n",
    "split_training_data(train_dir, val_dir, validation_split=0.1)\n",
    "\n",
    "# Hitung jumlah gambar setelah split\n",
    "print(\"\\nSetelah split:\")\n",
    "train_count_after = count_images_in_folder(train_dir)\n",
    "val_count_after = count_images_in_folder(val_dir)\n",
    "print(f\"  Training: {train_count_after} gambar\")\n",
    "print(f\"  Validation: {val_count_after} gambar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisikan transformasi\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Definisikan path ke dataset\n",
    "train_dir = f'{home_path}/datasets/training'\n",
    "val_dir = f'{home_path}/datasets/validation'\n",
    "\n",
    "# Buat dataset menggunakan ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "# Buat dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs=50):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0  # Track correct predictions for training accuracy\n",
    "\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            # Calculate training accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_accuracy = train_correct.double() / len(train_dataloader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_dataloader.dataset)\n",
    "        val_accuracy = val_correct.double() / len(val_dataloader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "        # Print training and validation metrics\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Train Loss: {train_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pretrained EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = EfficientNet_V2_S_Weights\n",
    "model = efficientnet_v2_s(weights=weights, progress=True)\n",
    "\n",
    "num_classes = 3  # Jumlah kelas Anda\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "summary(model, input_size=(1, 3, 224, 224))\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
    "    model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pretrained Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = None\n",
    "model = swin_t(weights=weights, progress=True)\n",
    "\n",
    "num_classes = 3  # Jumlah kelas Anda\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "summary(model, input_size=(1, 3, 224, 224))\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
    "    model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_base_model(model_name, img_height, img_width, trainable_percentage, skema):\n",
    "    \n",
    "\n",
    "#     # Build model\n",
    "#     model = build_model(base_model, train_generator.num_classes, dropout_rate)\n",
    "    \n",
    "#     # Compile model\n",
    "#     compile_model(model, learning_rate)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "# def build_model(base_model, num_classes,dropout_rate):\n",
    "#     model = models.Sequential([\n",
    "#         base_model,\n",
    "#         layers.GlobalAveragePooling2D(),\n",
    "#         layers.Dense(256, activation='relu'),\n",
    "#         layers.Dropout(dropout_rate),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.Dropout(dropout_rate),\n",
    "#         layers.Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# def compile_model(model, learning_rate):\n",
    "    \n",
    "\n",
    "# def train_model(model, train_generator, val_generator, epochs_size):\n",
    "#     start_time = time.time()\n",
    "#     history = model.fit(\n",
    "#         train_generator,\n",
    "#         epochs=epochs_size,\n",
    "#         validation_data=val_generator\n",
    "#     )\n",
    "#     duration = time.time() - start_time\n",
    "#     return history, duration\n",
    "\n",
    "# def evaluate_model(model, test_generator):\n",
    "#     eval_result = model.evaluate(test_generator)\n",
    "#     return eval_result[1] * 100, eval_result[0]\n",
    "\n",
    "# def save_model(model, save_path, timestamp, model_name):\n",
    "#     model_save_path = os.path.join(save_path, f\"{model_name}_{timestamp}.keras\")\n",
    "#     model.save(model_save_path)\n",
    "#     return model_save_path\n",
    "\n",
    "# def save_results_to_csv(results, csv_save_path):\n",
    "#     df = pd.DataFrame(results)\n",
    "#     if os.path.exists(csv_save_path):\n",
    "#         existing_df = pd.read_csv(csv_save_path)\n",
    "#         df = pd.concat([existing_df, df], ignore_index=True)\n",
    "#     df.to_csv(csv_save_path, index=False)\n",
    "\n",
    "# def plot_training_metrics(history, image_path, timestamp):\n",
    "#     # Accuracy Plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "#     plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.title('Training and Validation Accuracy')\n",
    "#     plt.savefig(os.path.join(image_path, f\"training_accuracy_{timestamp}.png\"))\n",
    "#     plt.show()\n",
    "\n",
    "#     # Loss Plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(history.history['loss'], label='Training Loss')\n",
    "#     plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "#     plt.title('Training and Validation Loss')\n",
    "#     plt.savefig(os.path.join(image_path, f\"training_loss_{timestamp}.png\"))\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_confusion_matrix(model, test_generator, image_path, timestamp):\n",
    "#     y_true = test_generator.classes\n",
    "#     y_pred = np.argmax(model.predict(test_generator), axis=1)\n",
    "#     cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(test_generator.class_indices)))\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(test_generator.class_indices.keys()))\n",
    "#     disp.plot(cmap='Blues', xticks_rotation='vertical', ax=ax)\n",
    "\n",
    "#     plt.title('Confusion Matrix', fontsize=16)\n",
    "#     plt.tight_layout()\n",
    "#     confusion_matrix_path = os.path.join(image_path, f\"confusion_matrix_{timestamp}.png\")\n",
    "#     fig.savefig(confusion_matrix_path, bbox_inches='tight', dpi=300)\n",
    "#     plt.show()\n",
    "#     plt.close(fig)\n",
    "#     return confusion_matrix_path\n",
    "\n",
    "# def run_training(model):\n",
    "    \n",
    "#     # Train model\n",
    "#     history, training_duration = train_model(model, train_generator, val_generator, epochs_size)\n",
    "    \n",
    "#     # Evaluate model\n",
    "#     test_accuracy, test_loss = evaluate_model(model, test_generator)\n",
    "    \n",
    "#     # Save model\n",
    "#     timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#     save_model(model, save_model_path, timestamp,model_name)\n",
    "    \n",
    "#     # Save results\n",
    "#     results = {\n",
    "#         \"Timestamp\": [timestamp],\n",
    "#         \"Model\": [model_name],\n",
    "#         \"Skema\":[skema],\n",
    "#         \"Batch Size\": [batch_size],\n",
    "#         \"Epochs\": [epochs_size],\n",
    "#         \"Training Accuracy\": [max(history.history['accuracy']) * 100],\n",
    "#         \"Validation Accuracy\": [max(history.history['val_accuracy']) * 100],\n",
    "#         \"Testing Accuracy\": [test_accuracy],\n",
    "#         \"Training Loss\": [min(history.history['loss'])],\n",
    "#         \"Validation Loss\": [min(history.history['val_loss'])],\n",
    "#         \"Testing Loss\": [test_loss],\n",
    "#         \"Training Duration (seconds)\": [training_duration],\n",
    "#         \"Training Duration (minutes)\": [f\"{training_duration / 60:.2f}\"],\n",
    "#         \"Trainable Percentage\": [trainable_percentage_modifikasi * 100 if \"Modifikasi\" in model_name else trainable_percentage * 100],\n",
    "#         \"Total Parameter\": [f\"{model.count_params():,}\".replace(\",\", \".\")]\n",
    "#     }\n",
    "#     save_results_to_csv(results, os.path.join(result_csv_path, \"training_results_runpod.csv\"))\n",
    "\n",
    "#     show_classification_report(model, test_generator, result_csv_path, timestamp)\n",
    "\n",
    "#     return history, image_path, timestamp, model, training_duration, test_accuracy\n",
    "\n",
    "# def plot_history(history, image_path, timestamp, model, training_duration, test_accuracy):\n",
    "#     # Plot metrics\n",
    "#     plot_training_metrics(history, image_path, timestamp)\n",
    "    \n",
    "#     # Plot confusion matrix\n",
    "#     confusion_matrix_path = plot_confusion_matrix(model, test_generator, image_path, timestamp)\n",
    "    \n",
    "\n",
    "# def show_classification_report(model, test_generator, save_path, timestamp):\n",
    "#     y_true = test_generator.classes\n",
    "#     y_pred = np.argmax(model.predict(test_generator), axis=1)\n",
    "#     class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "#     print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet V2 Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet V2 Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet V2 Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer Tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
